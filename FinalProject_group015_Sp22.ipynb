{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEihCld3ByH0"
      },
      "source": [
        "# COGS 118A- Final Project Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKUR_Z2WByIF"
      },
      "source": [
        "# Names\n",
        "\n",
        "\n",
        "- Kimberly Liu\n",
        "- Ruoxuan Li\n",
        "- Juan Castillejos-Garcia\n",
        "- Manan Surana"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6ffkwspByII"
      },
      "source": [
        "#Abstract\n",
        "Our goal of the project is to find the best method to predict the readmission rates of diabetes patients in the  US by comparing different classification methods learned in class. The data used represents 10 years (1999–2008) of clinical care at 130 hospitals throughout the United States. It contains over 50 attributes such as patient number, race, gender, and age, etc. and 3 classification labels: >30(readmitted after 30 days), <30(readmitted within 30 days), and no (no readmission). We selected a range of diffferent attributes to create various datasets and use them to predict the readmission rates via ML methods such as SoftMax regression, multi-class SVM, KNN, Decision tree and Multilayer perceptron. We compare the models' performace by looking at the rates of Type 1, Type 2 errors and overall accuracy. However, our experiments indicate that the features of the dataset are  not  ideal for the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJFAXWfrByIU"
      },
      "source": [
        "# Background\n",
        "Diabetes is associated with an increased risk of heart disease, stroke, high blood pressure and narrowing of blood vessels<a name = \"https://www.mayoclinic.org/diseases-conditions/type-2-diabetes/symptoms-causes/syc-20351193#:~:text=Potential%20complications%20of%20diabetes%20and,damage%20(neuropathy)%20in%20limbs.\"></a>[<sup>[1]</sup>](#abc). The high readmission rates of diabetes patients bring heavy burden to the medical systems in the US.\n",
        "According to one of the research, thirty-day readmission rates for hospitalized patients with diabetes are reported to be between 14.4 and 22.7%, much higher than the rate for all hospitalized patients (8.5–13.5%)<a name=\"https://clindiabetesendo.biomedcentral.com/articles/10.1186/s40842-016-0040-x\"></a>[<sup>[2]</sup>](#admonishnote). In order to provide better care for patients who are diagnosed with diabetes, some researchers have attempted to predict the readmission rates with certain attributes associated with patients.\n",
        "Prior study done by _Strack, Beata, et al_. suggests that the relationship between the probability of readmission and the HbA1c measurement depends on the primary diagnosis<a name=\"https://www.hindawi.com/journals/bmri/2014/781670/\"></a>[<sup>[3]</sup>](#wfds) using multivariable logistic regression. \n",
        "In this project, we want to explore machine learning algorithms that might help us better predict the readmission rates to which could be useful for small town policy makers to allocate medical resources and personnel more appropriately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCLI0FxCByIX"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "The U.S has a problem of access to health care<a name=\"ander\"></a>[<sup>[4]</sup>](#ander). The number of health care professional required to service the aging population is greater than the number of well-trained medical professionals being supplied. Rural areas are especially vulnerable to not having many qualified doctors and nurses<a name=\"ziller\"></a>[<sup>[5]</sup>](#ziller). Our goal of this project is improving patient outcomes by exploring the accuracy of 4 machine learning models in predicting readmission rate of patients using historic data collected over 10 years. Most hospitals keep a clinical history for every patient admitted which in rural areas can be used as input to an efficient machine learning algorithm to predict staffing and funding needs. Our project seeks to use features found in such clinical histories to label a patient into each of the 3 readmission categories. For any given time interval of a hospital, a \"future prediction set\" could be put through the algorithm with best performance to label each patient. Taking the count of each of the patients in the predcition set labeled as needing readmission, the readmission rate for the following 30 days could be computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGRPcRO_ByIb"
      },
      "source": [
        "# Data\n",
        "For this project, we use \"Diabetes 130-US hospitals\" dataset training and testing dataset. The dataset comes from The UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008\n",
        "\n",
        "For our experiments, we split our dataset into a training dataset and a test dataset. The training dataset is used to train the different machine learning algorithms that we used. After traning our models or classifiers with the training dataset, we will need a testing dataset to test them. Due to the size of the dataset cross validation was hard to implement, but hyperpareter tunning used gridsearch was performed.\n",
        "The dataset represents clinical care provided between 1999 to 2008 at 130 US hospitals. The dataset contains of categorical and quantitative variables\n",
        "There are 55 different variables that represent each observation. There are 100,000 observations present in the dataset. Each observation consists of the following atributes:\n",
        "\n",
        "- 'encounter_id'  and 'patient_nbr' are numerical variables(feature) that the hospital used to identify the patitent during a particualr visit. These are removed from the datasets as they are not expected to contribute any significant information to the model. \n",
        "- 'race' is  a nominal variable with values: Caucasian, Asian, African American, Hispanic, and other. Some patients had these value missing and were removed during data cleaning.\n",
        "-'gender' is a nominal variable with values: male, female, and unknown/invalid. Patients whose gender was in the third class were removed from the training and testing datasets.\n",
        "- 'age' is a nominal variable grouped in 10-year intervals with ages ranging in [0,100). This variable did not have any missing values, but was converted to an integer variable by selecting the lower bound of the corresponding interval as its value.\n",
        "- 'weight' is a numerical variable of the patients weight in pounds. However, the majority of this subjects had this value missing. As such, it was decided to not include this variable into the datasets.\n",
        "- 'admission_type_id' and 'admission_source_id' are nominal variable that are encoded to represent the way a patient was admitted to the hospital.\n",
        "- 'discharge_disposition_id' is a nominal variable that encodes how a patient was discharged. Although no values are explicitly missing, a possible value of the variable is \"not available\".\n",
        "- 'time_in_hospital' is a numerical variable giving the number of days between patient admission and discharge.\n",
        "- 'payer_code' is a nominal varible representing how the patient would pay for the visit. This variable was also removed from our dataset.\n",
        "- 'medical_specialty' is a nominal variable indentifying the specialty of the admitting physician. This variable also had about 50% missing values, so it was not considered during our experiments.\n",
        "- 'num_lab_procedures' is a numerical variable representing this number of of lab tested performed during the visit.\n",
        "- 'num_procedures' is a numerical variable with the number of procedures(besides lab tests) that were performed during the visit.\n",
        "- 'num_medications' is a numerical variable representing the number of medications administered during the visit.\n",
        "- 'number_outpatient' is a numerical variable representing the number of outpatient visits by the subject in the year preceding the observation in the dataset.\n",
        "- 'number_emergency' is a numerical variable representing the number of emergency visits by the subject in the year preceding the observation in the dataset.\n",
        "- 'number_inpatient'is a numerical variable representing the number of inpatient visits by the subject in the year preceding the observation in the dataset.\n",
        "- 'diag_1', 'diag_2', 'diag_3' are nominal variables that encode the patient's primary,secondary and additional secondary diagnosis(if applicable). Each diag has 848,923, and 954 distinct values.\n",
        "- 'number_diagnoses' is a numerical value representing the number of diagnoses entered into the system.\n",
        "- 'max_glu_serum' is a nominal variable with values: “>200,” “>300,” “normal,” and “none” if not measured.\n",
        "- 'A1Cresult' is a nominal variable with values: “>8” if the result was greater than 8%, “>7” if the result was greater than 7% but less than 8%, “normal” if the result was less than 7%, and “none” if not measured. A lot of patients had the previous 2 features as not measured. The patients were not immediately removed, and later experiments showed no significat difference in performance based on the inclusion of these features.\n",
        "- 'change' is a nominal variable that indicates if there was a change in the dosage or generic name of a diabetic medication \n",
        "- 'diabetesMed' is a nominal variable that indicated if there was a diabetic medication prescribed.\n",
        "- There are also 24 features for medications with the generic names of the medication. The nominal values of each drug indicates if the dosage was increased during the visit, decreased during the visit, if it did not change or if the drug was not prescribed.\n",
        "-'readmitted' is a nominal variable that serves as our label. \n",
        "\n",
        "After cleaning and removing features, we used a datset with 99,292 patient observations.\n",
        "More models based on feature selection were tested and some results about a dataset with the same features but with a size of 17,018 will mentioned in our results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qEg41OEFpo3"
      },
      "source": [
        "######Data Exploration and data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVfOSa241i-n"
      },
      "source": [
        "The original link https://archive.ics.uci.edu/ml/machine-learning-databases/00296/ only allows for donwloading the zip file to our local enviroment. To get around this problem and allow for real time collaboration we are using google drive to store the original data files and the post processing data files. All 4 menbers have acess to the shared drive and the notebook, but if the grader tries to run the notebook an error may occur. We include comments of how to run the code locally if the files datafile is in the same directory.\n",
        "\n",
        "All of the code has been moved to a separate file. It can be accessed using the following link:\n",
        "https://colab.research.google.com/drive/1CIAA-88JfMoLjjri-PCLm7pQTZiFUxsd?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7gl7bGEByIf"
      },
      "source": [
        "# Proposed Solution\n",
        "\n",
        "\n",
        "We plan to try out 4 different machine learning algorithms to the problem. Since this is a multi-class(3 classes) classification problem, all of our models should be multi-class classifiers, and we will use type I and type II errors of each class type to test how good our models behave. Here are the 4 models we will use: parametric models(Softmax Regression, Multi-class SVM) and non-parametric models(KNN, Decision Tree or Random Forest). \n",
        "\n",
        "- Softmax Regression  \n",
        "Softmax regression is a generalization of logistic regression to support multi-class classification problems. It utilizes softmax function to compute the probability of the sample belonging to each class. For implementation, we can use sklearn.linear_model.LogisticRegression(multi_class=‘multinomial’).\n",
        "- Multi-class SVM  \n",
        "SVM is a model that tries to find a hyperplane(decision boundary) that maximizes the distance between the data points and the class boundary. Since SVM does not support multi-class classification naturally, we can break down our problem to multipale binary classification problems. Since we have 3 classes, we will train 3 SVMs and each SVM predicts the membership of that sample in one of the classes. Regarding implementation details, we can use sklearn.svm.SVC(), where we will try to use different kernels: ‘linear’, ‘poly’, ‘rbf’ and use cross validation(GridSearchCV) to select the best kernel.\n",
        "- KNN  \n",
        "K-nearest neighbors classifer first calulates the first k closest neighbors of the data point and predict the label that has the most occurence in the first k nearest neighbors' labels. We will use sklearn.neighbors.KNeighborsClassifier() to implement this and use cross validation(KFold) to select our hyperparameter k.\n",
        "- Decision Tree or Random Forest  \n",
        "Decision tree is a way to orgnize our data in an hierarchical way and we can have any decision boundary(linear or non-linear) using decision trees. When building the tree, we select the splits that have the most information gain. We will use sklearn.tree.DecisionTreeClassifier() to implement this algorithm. Still, we are going to experiment with the criterion('gini' or 'entropy') also the max_depth of the tree to prevent overfitting.\n",
        "\n",
        "As we could find not paper/model that tried to solve our problem online, we will use the current state-of-the-art model, Deep Neural Network, specifically Multilayer Perceptrons, as our benchmark model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hzG5IubByIj"
      },
      "source": [
        "# Evaluation Metrics\n",
        "For resource allocation the main 2 cases of concern are having many staff and not enough customers or having too many customers and not enough staff. In a medical setting not having enough doctors and nurses is the worst of the previous 2 cases. Hence the algorithm minimizing the ratio of false negatives will be considered better. Note a false negative is incorrectly predicting the patient will not need readmission (Type 2 error).\n",
        "If $y$ is the number of patients incorrectly labeled as not needed readmission and $x$ is the number of patients correctly label as needing readmission, then $\\text{rate of false negatives} = \\frac{y}{x+y}$. \n",
        "\n",
        "From a purely finantial point of view being overstaffed is bad for business. Thus, we'll also look at the rate the algorithms incorrectly label a patient as needing readmission within 30 days (Type 1 error).\n",
        "We will also look at accuracy by computing the ratio between the number of incorrect predictions divided by the number of data points we are trying to classify and see if for these 3 different metrics there's a difference in the algorithm that could be considered better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcY4YxKLByIm"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Feaure Selection"
      ],
      "metadata": {
        "id": "06AG33I7S4D5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our experiments indicate the data  cannot be used to reliably make predictions abuut readmission rate too far off from random chance. We suspect this may have to do with how the features were recorded. None of the 4 models showed a significat difference from one another in accuracy even after hyperparameter tuning."
      ],
      "metadata": {
        "id": "418UyD50KgN8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzp1Q_VvPULC"
      },
      "source": [
        "For our experiments, we used the variables: 'race','gender', 'age', 'time_in_hospital', 'num_lab_procedures','num_procedures', 'num_medications' ,'number_outpatient','number_emergency', 'number_inpatient', 'number_diagnoses''max_glu_serum', 'A1Cresult' 'metformin', 'repaglinide', 'nateglinide' 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide' 'pioglitazone', 'rosiglitazone', 'acarbose' 'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone' 'change', 'diabetesMed'\n",
        "\n",
        "'encounter_id', 'patient_nbr' can be considered clerical variables, so they were not included in any of the experiments.\n",
        "'Weight was not used because of the large percent of missing entries that did not make sense to use average or mode to fill those in. \n",
        "\n",
        "We manually performed feature selection by looking at possible combination of the features. We explored using only the numerical features and explored looking at using ony categorical features. One-hot encoding wad used for the categorical feathres like medications. Some expreiments included adding/removing certain drugs to see if the performace improved. \n",
        "We tried looking at the number of diagnosis instead of the type of diagnosis, but we also tested the algorithms by encoding the diagnoses. 'max_glu_serum', 'A1Cresult' were not originally included because many patients did not take those tests, but were used later to reduce the size of the dataset as these values are of high importance to measure the progression of diabetes. Variables that had a very skewed distribution like race were removed and accuracy was not improved.\n",
        "In general we could not pass the threshold of 0.5716 accuracy on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxRu5OJ8WdQY"
      },
      "source": [
        "The plots for the distribution of each feature can be found: https://colab.research.google.com/drive/1CIAA-88JfMoLjjri-PCLm7pQTZiFUxsd?usp=sharing\n",
        "\n",
        "These plots guided or hand-design for feature selection. We also looked at the coefficents of softmax regression, in deciding what features to remove if the coefficents for $w$ were 0.\n",
        "\n",
        "One of our concerns was that perhaps linear decision boundaries were ill suitted for the task and used SVD to try to visualize the data. We got the following interesting plot below"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_working_2labels = X_temp.copy(deep=True)\n",
        "X_working_2labels = pd.concat([X_working_2labels, X_temp_2['readmitted']], axis=1)\n",
        "X_working_2labels[\"readmitted\"] = X_working_2labels[\"readmitted\"].apply(reafun1)"
      ],
      "metadata": {
        "id": "MgaFVmqxUqCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from pandas import DataFrame\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X_working_2labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGi2j3IxOgC9",
        "outputId": "c7849010-6151-4218-a402-97c3fc380357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(n_components=2)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['pca_%i' % i for i in range(2)]\n",
        "df_pca = DataFrame(pca.transform(X_working_2labels), columns=columns, index=X_working_2labels.index)\n",
        "df_pca.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ry-7b4XPPNVP",
        "outputId": "e6f3c1c6-6bef-49af-d7f8-0d36462a8463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       pca_0      pca_1\n",
              "0  -7.351454  61.202652\n",
              "1  13.412185  51.634831\n",
              "2 -34.264634  39.285758\n",
              "3  -0.764628  30.987913\n",
              "4   5.509741  21.597331"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6168f98-0d22-4c82-b8f5-41c8a1d25b96\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pca_0</th>\n",
              "      <th>pca_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-7.351454</td>\n",
              "      <td>61.202652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.412185</td>\n",
              "      <td>51.634831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-34.264634</td>\n",
              "      <td>39.285758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.764628</td>\n",
              "      <td>30.987913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.509741</td>\n",
              "      <td>21.597331</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6168f98-0d22-4c82-b8f5-41c8a1d25b96')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6168f98-0d22-4c82-b8f5-41c8a1d25b96 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6168f98-0d22-4c82-b8f5-41c8a1d25b96');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_pca.plot(x ='pca_0', y='pca_1', kind = 'scatter')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "d7cKcIrsPlab",
        "outputId": "3f03727d-b921-4c64-91ff-23deec41f1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this reason, we explore nonlinear kernels for SVM and ran more experiments on KNN and decision trees."
      ],
      "metadata": {
        "id": "B1cVsj0CU5-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiments and hyperparameter tuning"
      ],
      "metadata": {
        "id": "xr-F38FqZa6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we look at accuracy of the models, we should keep in mind that the proabilities of each label being selected at random are 'No': 0.5359, '<30' 0.1123, '>30':0.3519"
      ],
      "metadata": {
        "id": "LIlrwj-yaRPY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTM2Tk2yFHXL"
      },
      "source": [
        "- **Softmax Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl7ErRlQaZYg"
      },
      "source": [
        "The dataset was preprocessed using standardscaler. By running the GridSearchCV on different hyperparameters C, the parameter set that returns the best performance is {'logistic__C': 1.0, 'logistic__penalty': 'l2'}. \n",
        "\n",
        "The corresponding accuracy is 0.56. Type I error rate for label 0: 'no readmission' is approximately 0.42 abd Type II error rate is approximately 0.32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HyPdx47cuKW",
        "outputId": "d44ee7c0-1266-428c-faaa-c4683ca37cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# import data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = pd.read_csv (r'/content/drive/Shared drives/cogs118A/dataset_diabetes/X_multy_v3.csv')\n",
        "data.drop(columns = 'Unnamed: 0', inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vPAgBOsI8MU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "821fb181-e211-4c82-af98-f75b4adb79a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "# import pakacages\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlD2biEFJVLn"
      },
      "outputs": [],
      "source": [
        "# split train and test data\n",
        "X = data.iloc[:, : -1]\n",
        "y = data.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "rui1i28-LH58",
        "outputId": "7c864ed9-77d4-457c-f1eb-bd464787a332"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d8c24aa5-9caa-4a5d-8aa6-39e20b5592b2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_logistic__C</th>\n",
              "      <th>param_logistic__penalty</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.872603</td>\n",
              "      <td>0.043940</td>\n",
              "      <td>0.026201</td>\n",
              "      <td>0.004486</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'logistic__C': 0.0001, 'logistic__penalty': '...</td>\n",
              "      <td>0.558656</td>\n",
              "      <td>0.559781</td>\n",
              "      <td>0.558881</td>\n",
              "      <td>0.560156</td>\n",
              "      <td>0.557798</td>\n",
              "      <td>0.559054</td>\n",
              "      <td>0.000838</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.984148</td>\n",
              "      <td>0.599306</td>\n",
              "      <td>0.045574</td>\n",
              "      <td>0.026290</td>\n",
              "      <td>0.001</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'logistic__C': 0.001, 'logistic__penalty': 'l2'}</td>\n",
              "      <td>0.561281</td>\n",
              "      <td>0.564281</td>\n",
              "      <td>0.566157</td>\n",
              "      <td>0.568332</td>\n",
              "      <td>0.564924</td>\n",
              "      <td>0.564995</td>\n",
              "      <td>0.002315</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.048634</td>\n",
              "      <td>1.086279</td>\n",
              "      <td>0.039726</td>\n",
              "      <td>0.023036</td>\n",
              "      <td>0.01</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'logistic__C': 0.01, 'logistic__penalty': 'l2'}</td>\n",
              "      <td>0.561656</td>\n",
              "      <td>0.564056</td>\n",
              "      <td>0.567432</td>\n",
              "      <td>0.569082</td>\n",
              "      <td>0.563724</td>\n",
              "      <td>0.565190</td>\n",
              "      <td>0.002687</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.750710</td>\n",
              "      <td>0.155046</td>\n",
              "      <td>0.024486</td>\n",
              "      <td>0.002039</td>\n",
              "      <td>0.1</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'logistic__C': 0.1, 'logistic__penalty': 'l2'}</td>\n",
              "      <td>0.561581</td>\n",
              "      <td>0.564131</td>\n",
              "      <td>0.567582</td>\n",
              "      <td>0.569157</td>\n",
              "      <td>0.564024</td>\n",
              "      <td>0.565295</td>\n",
              "      <td>0.002717</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.383697</td>\n",
              "      <td>0.535466</td>\n",
              "      <td>0.033066</td>\n",
              "      <td>0.011991</td>\n",
              "      <td>1.0</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'logistic__C': 1.0, 'logistic__penalty': 'l2'}</td>\n",
              "      <td>0.561506</td>\n",
              "      <td>0.564131</td>\n",
              "      <td>0.567582</td>\n",
              "      <td>0.569157</td>\n",
              "      <td>0.564099</td>\n",
              "      <td>0.565295</td>\n",
              "      <td>0.002731</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8c24aa5-9caa-4a5d-8aa6-39e20b5592b2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8c24aa5-9caa-4a5d-8aa6-39e20b5592b2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8c24aa5-9caa-4a5d-8aa6-39e20b5592b2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
              "0       0.872603      0.043940         0.026201        0.004486   \n",
              "1       1.984148      0.599306         0.045574        0.026290   \n",
              "2       3.048634      1.086279         0.039726        0.023036   \n",
              "3       1.750710      0.155046         0.024486        0.002039   \n",
              "4       2.383697      0.535466         0.033066        0.011991   \n",
              "\n",
              "  param_logistic__C param_logistic__penalty  \\\n",
              "0            0.0001                      l2   \n",
              "1             0.001                      l2   \n",
              "2              0.01                      l2   \n",
              "3               0.1                      l2   \n",
              "4               1.0                      l2   \n",
              "\n",
              "                                              params  split0_test_score  \\\n",
              "0  {'logistic__C': 0.0001, 'logistic__penalty': '...           0.558656   \n",
              "1  {'logistic__C': 0.001, 'logistic__penalty': 'l2'}           0.561281   \n",
              "2   {'logistic__C': 0.01, 'logistic__penalty': 'l2'}           0.561656   \n",
              "3    {'logistic__C': 0.1, 'logistic__penalty': 'l2'}           0.561581   \n",
              "4    {'logistic__C': 1.0, 'logistic__penalty': 'l2'}           0.561506   \n",
              "\n",
              "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
              "0           0.559781           0.558881           0.560156           0.557798   \n",
              "1           0.564281           0.566157           0.568332           0.564924   \n",
              "2           0.564056           0.567432           0.569082           0.563724   \n",
              "3           0.564131           0.567582           0.569157           0.564024   \n",
              "4           0.564131           0.567582           0.569157           0.564099   \n",
              "\n",
              "   mean_test_score  std_test_score  rank_test_score  \n",
              "0         0.559054        0.000838                5  \n",
              "1         0.564995        0.002315                4  \n",
              "2         0.565190        0.002687                3  \n",
              "3         0.565295        0.002717                2  \n",
              "4         0.565295        0.002731                1  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# run the logistic regresion through the Grisearch algorithm to test cifferent hyperparameters C\n",
        "# we can see that C = 0.5 and C = 1 return very similar performances\n",
        "scaler = StandardScaler()\n",
        "softReg = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')\n",
        "pipe = Pipeline([(\"scaler\", scaler), (\"logistic\", softReg)])\n",
        "param_grid = {'logistic__penalty':['l2'],'logistic__C':[0.0001, 0.001, 0.01, 0.1, 1.0]}\n",
        "gscv = GridSearchCV(pipe,param_grid)\n",
        "gscv.fit(X_train, y_train)\n",
        "cv_results = pd.DataFrame.from_dict(gscv.cv_results_)\n",
        "cv_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWhKjw-zgTMi",
        "outputId": "9de9d2b9-131d-47d7-ac37-2e98b669225f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'logistic__C': 1.0, 'logistic__penalty': 'l2'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gscv.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZVvDk5tJr1q",
        "outputId": "0c8283b9-89de-4652-b234-3909f18d8305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.58      0.90      0.71     17604\n",
            "         1.0       0.42      0.01      0.02      3635\n",
            "         2.0       0.50      0.24      0.32     11594\n",
            "\n",
            "    accuracy                           0.57     32833\n",
            "   macro avg       0.50      0.38      0.35     32833\n",
            "weighted avg       0.53      0.57      0.49     32833\n",
            "\n",
            "[[15824     7  1773]\n",
            " [ 2607    31   997]\n",
            " [ 8812    36  2746]]\n"
          ]
        }
      ],
      "source": [
        "# test it with logistic regression\n",
        "# the accuracy we got is around 0.56\n",
        "scaler = StandardScaler()\n",
        "softReg = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')\n",
        "pipe = Pipeline([(\"scaler\", scaler), (\"logistic\", softReg)])\n",
        "pipe.fit(X_train, y_train)\n",
        "pipe.score(X_test, y_test)\n",
        "expected_y  = y_test\n",
        "predicted_y = pipe.predict(X_test)\n",
        "print(metrics.classification_report(expected_y, predicted_y))\n",
        "print(metrics.confusion_matrix(expected_y, predicted_y))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KwIzV4C2sPA",
        "outputId": "73753810-5872-4a44-a4e1-376af868fa6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "false positive rate for label 0 is  0.42000000000000004\n",
            "false negative rate for label 0 is  0.35844155844155845\n"
          ]
        }
      ],
      "source": [
        "## calcualte the Type I (false positive rate) and Type II (false negative rate) for each labels\n",
        "confusion_matrix = metrics.confusion_matrix(expected_y, predicted_y)\n",
        "print(\"false positive rate for label 0 is \", (1 - 0.58))\n",
        "fnr = sum(confusion_matrix[0,1:])/sum(sum(confusion_matrix[:,1:]))\n",
        "print(\"false negative rate for label 0 is \", fnr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some feature selection was also performed on the softmax model, and the redundant features were reported as below. But removing these features didn't improve the accuracy of our model."
      ],
      "metadata": {
        "id": "IQu5fP9J_ctL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw95VmDBe_D9",
        "outputId": "6acc45d3-83a1-4108-bbd8-acf9ea325198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Redundant Feature Count: 3\n",
            "Redundant Feature Names: ['No.15', 'No.16', 'No.21']\n"
          ]
        }
      ],
      "source": [
        "## some feature selection code\n",
        "model = LogisticRegression(multi_class = 'multinomial', penalty ='l2', solver = 'lbfgs')\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "model.fit(X_train_std, y_train)\n",
        "coef = model.coef_[0]\n",
        "print(\"Redundant Feature Count:\", sum(model.coef_[0] == 0))\n",
        "print(\"Redundant Feature Names:\", list(pd.Series(X_train.columns)[list(coef==0)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MccfbuakwGC5"
      },
      "source": [
        "- MLP (a supplement to the softmax model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having the doubt the the dataset might not be modeled properly by a linear kernel. A multi-layer perceptron model was applied to the data to see if there is any improvement. The dataset was preprocessed using standardscaler. First the default mlp was applied on the dataset. The corresponding accuracy is 0.56. Type I error rate for label 0: 'no readmission' is approximately 0.41 abd Type II error rate is approximately 0.36. Later, we tried to perform a GridsearchCV to test different hyperparameters, but since the computational cost was huge, ultimately we did not complete running the search becasue perfomance on the other models did not support a hope of increase in performance."
      ],
      "metadata": {
        "id": "_Yh0RHt7vlcJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYDhfthawIdN"
      },
      "outputs": [],
      "source": [
        "## import \n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFvzZjdjwiKI"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "MLP = MLPClassifier()\n",
        "pipe = Pipeline([(\"scaler\", scaler), (\"mlpc\", MLP)])\n",
        "pipe.fit(X_train, y_train)\n",
        "pipe.score(X_test, y_test)\n",
        "expected_y  = y_test\n",
        "predicted_y = pipe.predict(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQbZ_CTvyErO",
        "outputId": "546732f8-bba5-4af2-d96b-ac989c4901df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.84      0.69     17604\n",
            "         1.0       0.27      0.04      0.07      3635\n",
            "         2.0       0.48      0.29      0.36     11594\n",
            "\n",
            "    accuracy                           0.56     32833\n",
            "   macro avg       0.45      0.39      0.38     32833\n",
            "weighted avg       0.51      0.56      0.51     32833\n",
            "\n",
            "[[14844   124  2636]\n",
            " [ 2365   139  1131]\n",
            " [ 7924   253  3417]]\n"
          ]
        }
      ],
      "source": [
        "print(metrics.classification_report(expected_y, predicted_y))\n",
        "print(metrics.confusion_matrix(expected_y, predicted_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bCw8LtiAtqj",
        "outputId": "aeaefd39-0f4e-4b70-90b7-96faf098f2d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5604117808302622\n",
            "false positive rate for 0 is  0.41000000000000003\n",
            "false negative rate for 0 is  0.35844155844155845\n"
          ]
        }
      ],
      "source": [
        "print(pipe.score(X_test, y_test))\n",
        "confusion_matrix = metrics.confusion_matrix(expected_y, predicted_y)\n",
        "print(\"false positive rate for label 0 is \", (1 - 0.59))\n",
        "fnr = sum(confusion_matrix[0,1:])/sum(sum(confusion_matrix[:,1:]))\n",
        "print(\"false negative rate for label 0 is \", fnr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below was used to tune the hyperparameters, but we had to kill it manually because it ran for too long."
      ],
      "metadata": {
        "id": "WWs35VNI-Pk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'mlpc__solver':['adam', 'lbfgs', 'sgd'],'mlpc__activation':['identity', 'logistic', 'tanh', 'relu'], 'mlpc__alpha':[0.0001, 0.001, 0.01]}\n",
        "gscv = GridSearchCV(pipe,param_grid)\n",
        "gscv.fit(X_train, y_train)\n",
        "cv_results = pd.DataFrame.from_dict(gscv.cv_results_)\n",
        "cv_results"
      ],
      "metadata": {
        "id": "4MREv43d4u02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXsIVnSPH0Jw"
      },
      "source": [
        "- **KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6JlodExJIwF"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afjWGq-pc0D2"
      },
      "outputs": [],
      "source": [
        "# test the data with KNN\n",
        "# \n",
        "\n",
        "K = []\n",
        "training = []\n",
        "test = []\n",
        "scores = {}\n",
        "f1_scores = {}\n",
        "  \n",
        "for k in range(1, 21):\n",
        "    scaler = StandardScaler()\n",
        "    Kneigh = KNeighborsClassifier(n_neighbors = k)\n",
        "    pipe = Pipeline([(\"scaler\", scaler), (\"knn\", Kneigh)])\n",
        "    \n",
        "    pipe.fit(X_train, y_train)\n",
        "  \n",
        "    training_score = pipe.score(X_train, y_train)\n",
        "    test_score = pipe.score(X_test, y_test)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "    macro = f1_score(y_test, y_pred, average='macro')\n",
        "    micro = f1_score(y_test, y_pred, average='micro')\n",
        "    weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "\n",
        "    K.append(k)\n",
        "  \n",
        "    training.append(training_score)\n",
        "    test.append(test_score)\n",
        "    scores[k] = [training_score, test_score]\n",
        "    f1_scores[k] = [macro, micro, weighted]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3cn6CmWVzka",
        "outputId": "7f7fabfc-c2f1-4113-cf1c-f29fc1f15048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : [0.9971046670367092, 0.45579142935461275]\n",
            "2 : [0.7292638653445146, 0.501720829653093]\n",
            "3 : [0.710196672617351, 0.506776718545366]\n",
            "4 : [0.673997509713617, 0.5186550117260074]\n",
            "5 : [0.6589057741640288, 0.5191423263180337]\n",
            "6 : [0.6453292128594789, 0.5289495324825633]\n",
            "7 : [0.6367482260459952, 0.528370846404532]\n",
            "8 : [0.6291873565460028, 0.5348582219108824]\n",
            "9 : [0.6224065767563269, 0.5379648524350501]\n",
            "10 : [0.6180110712731964, 0.5409191971492097]\n",
            "11 : [0.612340419148202, 0.543203484299333]\n",
            "12 : [0.6111552828575286, 0.5455182286114579]\n",
            "13 : [0.6075398670847147, 0.5462187433374958]\n",
            "14 : [0.6044795151442416, 0.5461882861754942]\n",
            "15 : [0.602874330548013, 0.5486553162976274]\n",
            "16 : [0.6012241407761892, 0.5506045746657326]\n",
            "17 : [0.5988538681948424, 0.5512137179057656]\n",
            "18 : [0.5972336818734154, 0.5510614320957573]\n",
            "19 : [0.596123554208734, 0.5510614320957573]\n",
            "20 : [0.5941733299329423, 0.5533761764078823]\n"
          ]
        }
      ],
      "source": [
        "# print the test\n",
        "for keys, values in scores.items():\n",
        "  print(keys, ':', values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "YNTmKVWUrvjI",
        "outputId": "d2842d45-ee9f-4de4-9d91-6d062a717633"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# display training scores with respect to different k values\n",
        "ax = sns.stripplot(K, training);\n",
        "ax.set(xlabel ='values of k', ylabel ='Training Score')  \n",
        "  \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "zcT6lQLhry2n",
        "outputId": "963d6806-f9ad-438d-939b-5c502b7f00a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# display testing scores with respect to different k values\n",
        "ax = sns.stripplot(K, test);\n",
        "ax.set(xlabel ='values of k', ylabel ='Test Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "5a4q_ae1r1Dr",
        "outputId": "bebc6047-1192-452a-e1bf-6e3b019e5f58"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# display both training & testing scores with respect to different k values\n",
        "plt.scatter(K, training, color ='k')\n",
        "plt.scatter(K, test, color ='g')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to tune the hyperparamaters for our KNN model we increased the value of K. We also rescaled the features as we know that KNN is a distance based algorithm. It is important to rescale so the units are not different during distance calculation. We used Standard Scaling, Robust Scaling and MinMax Scaling. The inital hyperparameter tuning was done manually and later supplemeted by gridsearch."
      ],
      "metadata": {
        "id": "6kebULPxHOyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for keys, values in scores.items():\n",
        "  print(keys, ':', values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5hztsSRp8u7",
        "outputId": "171211d0-8596-4c6b-ed7c-35dc4e33c665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 : [0.727013606564755, 0.5799957359973198]\n",
            "55 : [0.638923476199763, 0.6148082721651996]\n",
            "105 : [0.6303424893862795, 0.6165138732372918]\n",
            "155 : [0.626292023582712, 0.6146559863551915]\n",
            "205 : [0.6218365111987879, 0.6129199281210976]\n",
            "255 : [0.6209214059616855, 0.6127676423110895]\n",
            "305 : [0.6191362006630763, 0.6102396978649529]\n",
            "355 : [0.618221095425974, 0.6098742119209332]\n",
            "405 : [0.6173809988148637, 0.6094478116529102]\n",
            "455 : [0.6150557314091121, 0.6081381536868394]\n",
            "505 : [0.6136755726908595, 0.6070112386927786]\n",
            "555 : [0.6125954484765748, 0.6070416958547803]\n",
            "605 : [0.6122054036214165, 0.6067675813967655]\n",
            "655 : [0.6117403501402662, 0.6060975238327293]\n",
            "705 : [0.6107352345519734, 0.6047878658666586]\n",
            "755 : [0.6097151172384824, 0.6055492949166996]\n",
            "805 : [0.6098651344904664, 0.6061584381567325]\n",
            "855 : [0.6096851137880857, 0.605092437486675]\n",
            "905 : [0.6089800327037609, 0.6055492949166996]\n",
            "955 : [0.6079599153902698, 0.6053056376206865]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculated the model performance for values of k starting from 5 to 955 in an interval of 50. We found that the best value for K will be in between 100 and 130. The best result we got was a test score of 0.6165 for k=105. The model also only used Standard Scaling. We will also have to run the model on the multi-label datset. Therefore the performance will be lower. "
      ],
      "metadata": {
        "id": "YST6Yr4WI-A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After finding the range for the best k values, we ran the model using Standard, Robust and MinMax Scaling. We found the best result by scaling the features using Robust Scaling. "
      ],
      "metadata": {
        "id": "GtxsaZ0uKe5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was preprocessed using Robust Scalar. We ran GridSearchCV on different hyperparameters n_neighbors and p. We used the range 100 to 130 to find the best k and we used both Euclidean and Manhattan distance for our model "
      ],
      "metadata": {
        "id": "oYQnyR4hLe93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_neighbors = list(range(100,130))\n",
        "p=[1,2]\n",
        "hyperparameters = dict(n_neighbors=n_neighbors, p=p)\n",
        "clf = GridSearchCV(Kneigh, hyperparameters, cv=10)\n",
        "best_model = clf.fit(X,y)"
      ],
      "metadata": {
        "id": "n_CCnvD8EKTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame.from_dict(clf.cv_results_)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3LsMqvuwFIlO",
        "outputId": "39f04f65-c63f-437b-cf33-7076beeee270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
              "0        0.029992      0.004034        19.583461        6.283864   \n",
              "1        0.031939      0.000501        10.819235        3.837921   \n",
              "2        0.032260      0.000888        18.446579        5.384533   \n",
              "3        0.032137      0.000622        10.734623        3.914093   \n",
              "4        0.033434      0.002098        19.247969        5.107660   \n",
              "5        0.032943      0.000539        10.866465        3.908196   \n",
              "6        0.034106      0.001616        19.334142        5.272047   \n",
              "7        0.033272      0.001580        10.856128        3.897087   \n",
              "8        0.033004      0.000615        19.182827        5.265523   \n",
              "9        0.032478      0.001373        10.978729        4.052047   \n",
              "10       0.031386      0.000550        19.395233        5.136494   \n",
              "11       0.031246      0.001019        10.942338        4.011360   \n",
              "12       0.030954      0.000456        18.985566        5.250723   \n",
              "13       0.031249      0.000844        10.944542        3.924248   \n",
              "14       0.032097      0.001430        19.467301        5.485797   \n",
              "15       0.031195      0.000722        10.851434        3.907510   \n",
              "16       0.031097      0.000561        19.234978        5.670268   \n",
              "17       0.031038      0.000718        10.880824        3.923545   \n",
              "18       0.031265      0.000813        19.494126        5.427829   \n",
              "19       0.031351      0.001097        10.851609        3.910496   \n",
              "20       0.031380      0.001091        18.690243        5.247739   \n",
              "21       0.031162      0.000858        10.805196        3.923366   \n",
              "22       0.030878      0.000312        18.862524        5.248098   \n",
              "23       0.031128      0.000392        10.835748        3.923821   \n",
              "24       0.031103      0.000703        19.340723        5.417679   \n",
              "25       0.031559      0.001397        11.179422        4.699272   \n",
              "26       0.038680      0.001158        18.740545        4.830330   \n",
              "27       0.039666      0.004799        10.934129        3.862349   \n",
              "28       0.038497      0.001357        17.596461        5.229767   \n",
              "29       0.038245      0.001348        10.928183        3.831038   \n",
              "30       0.038136      0.000929        17.596255        5.324058   \n",
              "31       0.038125      0.000618        10.955339        3.974994   \n",
              "32       0.037999      0.000671        17.600643        5.231226   \n",
              "33       0.038171      0.001100        10.975556        3.899864   \n",
              "34       0.038436      0.001455        17.825837        5.520120   \n",
              "35       0.036961      0.003540        10.934888        3.851092   \n",
              "36       0.036069      0.000879        18.808737        5.484093   \n",
              "37       0.036368      0.001559        10.833655        3.924031   \n",
              "38       0.035087      0.000586        18.242149        5.573804   \n",
              "39       0.036065      0.001829        10.833819        3.931543   \n",
              "40       0.036366      0.001949        18.798248        5.151254   \n",
              "41       0.037036      0.001841        10.929703        3.894919   \n",
              "42       0.035491      0.001094        18.067401        5.308730   \n",
              "43       0.035679      0.000924        10.937693        3.891505   \n",
              "44       0.036137      0.001447        18.407905        5.660284   \n",
              "45       0.036364      0.001522        11.258909        3.995546   \n",
              "46       0.036839      0.002120        19.423646        5.430554   \n",
              "47       0.036172      0.001303        11.281178        3.947256   \n",
              "48       0.036589      0.001166        19.101065        4.889240   \n",
              "49       0.035816      0.001224        11.201558        4.046468   \n",
              "50       0.035678      0.001777        18.911710        5.272432   \n",
              "51       0.036127      0.004063        12.011537        3.858700   \n",
              "52       0.035328      0.000816        18.503945        5.186942   \n",
              "53       0.035407      0.003116        11.519618        3.565437   \n",
              "54       0.038202      0.001662        18.486252        5.260859   \n",
              "55       0.038634      0.000793        11.176421        3.851463   \n",
              "56       0.037639      0.000680        17.799359        5.427855   \n",
              "57       0.038900      0.001378        11.270238        4.138703   \n",
              "58       0.038648      0.000555        17.931905        5.292418   \n",
              "59       0.039802      0.004395        11.271419        3.989344   \n",
              "\n",
              "   param_n_neighbors param_p                        params  split0_test_score  \\\n",
              "0                100       1  {'n_neighbors': 100, 'p': 1}           0.351859   \n",
              "1                100       2  {'n_neighbors': 100, 'p': 2}           0.351859   \n",
              "2                101       1  {'n_neighbors': 101, 'p': 1}           0.351859   \n",
              "3                101       2  {'n_neighbors': 101, 'p': 2}           0.351859   \n",
              "4                102       1  {'n_neighbors': 102, 'p': 1}           0.351859   \n",
              "5                102       2  {'n_neighbors': 102, 'p': 2}           0.351859   \n",
              "6                103       1  {'n_neighbors': 103, 'p': 1}           0.351859   \n",
              "7                103       2  {'n_neighbors': 103, 'p': 2}           0.351859   \n",
              "8                104       1  {'n_neighbors': 104, 'p': 1}           0.351859   \n",
              "9                104       2  {'n_neighbors': 104, 'p': 2}           0.351859   \n",
              "10               105       1  {'n_neighbors': 105, 'p': 1}           0.351859   \n",
              "11               105       2  {'n_neighbors': 105, 'p': 2}           0.351859   \n",
              "12               106       1  {'n_neighbors': 106, 'p': 1}           0.351859   \n",
              "13               106       2  {'n_neighbors': 106, 'p': 2}           0.351859   \n",
              "14               107       1  {'n_neighbors': 107, 'p': 1}           0.351859   \n",
              "15               107       2  {'n_neighbors': 107, 'p': 2}           0.351859   \n",
              "16               108       1  {'n_neighbors': 108, 'p': 1}           0.351859   \n",
              "17               108       2  {'n_neighbors': 108, 'p': 2}           0.351859   \n",
              "18               109       1  {'n_neighbors': 109, 'p': 1}           0.351859   \n",
              "19               109       2  {'n_neighbors': 109, 'p': 2}           0.351859   \n",
              "20               110       1  {'n_neighbors': 110, 'p': 1}           0.351859   \n",
              "21               110       2  {'n_neighbors': 110, 'p': 2}           0.351859   \n",
              "22               111       1  {'n_neighbors': 111, 'p': 1}           0.351859   \n",
              "23               111       2  {'n_neighbors': 111, 'p': 2}           0.351859   \n",
              "24               112       1  {'n_neighbors': 112, 'p': 1}           0.351859   \n",
              "25               112       2  {'n_neighbors': 112, 'p': 2}           0.351859   \n",
              "26               113       1  {'n_neighbors': 113, 'p': 1}           0.351859   \n",
              "27               113       2  {'n_neighbors': 113, 'p': 2}           0.351859   \n",
              "28               114       1  {'n_neighbors': 114, 'p': 1}           0.351859   \n",
              "29               114       2  {'n_neighbors': 114, 'p': 2}           0.351859   \n",
              "30               115       1  {'n_neighbors': 115, 'p': 1}           0.351859   \n",
              "31               115       2  {'n_neighbors': 115, 'p': 2}           0.351859   \n",
              "32               116       1  {'n_neighbors': 116, 'p': 1}           0.351859   \n",
              "33               116       2  {'n_neighbors': 116, 'p': 2}           0.351859   \n",
              "34               117       1  {'n_neighbors': 117, 'p': 1}           0.351859   \n",
              "35               117       2  {'n_neighbors': 117, 'p': 2}           0.351859   \n",
              "36               118       1  {'n_neighbors': 118, 'p': 1}           0.351859   \n",
              "37               118       2  {'n_neighbors': 118, 'p': 2}           0.351859   \n",
              "38               119       1  {'n_neighbors': 119, 'p': 1}           0.351859   \n",
              "39               119       2  {'n_neighbors': 119, 'p': 2}           0.351859   \n",
              "40               120       1  {'n_neighbors': 120, 'p': 1}           0.351859   \n",
              "41               120       2  {'n_neighbors': 120, 'p': 2}           0.351859   \n",
              "42               121       1  {'n_neighbors': 121, 'p': 1}           0.351859   \n",
              "43               121       2  {'n_neighbors': 121, 'p': 2}           0.351859   \n",
              "44               122       1  {'n_neighbors': 122, 'p': 1}           0.351859   \n",
              "45               122       2  {'n_neighbors': 122, 'p': 2}           0.351859   \n",
              "46               123       1  {'n_neighbors': 123, 'p': 1}           0.351859   \n",
              "47               123       2  {'n_neighbors': 123, 'p': 2}           0.351859   \n",
              "48               124       1  {'n_neighbors': 124, 'p': 1}           0.351859   \n",
              "49               124       2  {'n_neighbors': 124, 'p': 2}           0.351859   \n",
              "50               125       1  {'n_neighbors': 125, 'p': 1}           0.351859   \n",
              "51               125       2  {'n_neighbors': 125, 'p': 2}           0.351859   \n",
              "52               126       1  {'n_neighbors': 126, 'p': 1}           0.351859   \n",
              "53               126       2  {'n_neighbors': 126, 'p': 2}           0.351859   \n",
              "54               127       1  {'n_neighbors': 127, 'p': 1}           0.351859   \n",
              "55               127       2  {'n_neighbors': 127, 'p': 2}           0.351859   \n",
              "56               128       1  {'n_neighbors': 128, 'p': 1}           0.351859   \n",
              "57               128       2  {'n_neighbors': 128, 'p': 2}           0.351859   \n",
              "58               129       1  {'n_neighbors': 129, 'p': 1}           0.351859   \n",
              "59               129       2  {'n_neighbors': 129, 'p': 2}           0.351859   \n",
              "\n",
              "    split1_test_score  split2_test_score  split3_test_score  \\\n",
              "0            0.384824           0.535933           0.253593   \n",
              "1            0.384824           0.535933           0.253493   \n",
              "2            0.384724           0.535933           0.253593   \n",
              "3            0.384724           0.535933           0.253392   \n",
              "4            0.384925           0.535933           0.253493   \n",
              "5            0.384824           0.535933           0.253794   \n",
              "6            0.384824           0.535933           0.253694   \n",
              "7            0.384724           0.535933           0.253694   \n",
              "8            0.384824           0.535933           0.253593   \n",
              "9            0.384724           0.535933           0.253794   \n",
              "10           0.384824           0.535933           0.253493   \n",
              "11           0.384824           0.535933           0.253593   \n",
              "12           0.384724           0.535933           0.253794   \n",
              "13           0.384724           0.535933           0.253593   \n",
              "14           0.384824           0.535933           0.253593   \n",
              "15           0.384824           0.535933           0.253392   \n",
              "16           0.384824           0.535933           0.253593   \n",
              "17           0.385025           0.535933           0.253593   \n",
              "18           0.384824           0.535933           0.253593   \n",
              "19           0.385025           0.535933           0.253392   \n",
              "20           0.384724           0.535933           0.253493   \n",
              "21           0.385025           0.535933           0.253694   \n",
              "22           0.384824           0.535933           0.253493   \n",
              "23           0.385126           0.535933           0.253593   \n",
              "24           0.384824           0.535933           0.253493   \n",
              "25           0.385025           0.535933           0.253493   \n",
              "26           0.384824           0.535933           0.253493   \n",
              "27           0.385126           0.535933           0.253493   \n",
              "28           0.384925           0.535933           0.253593   \n",
              "29           0.385025           0.535933           0.253895   \n",
              "30           0.384925           0.535933           0.253493   \n",
              "31           0.385126           0.535933           0.253895   \n",
              "32           0.384724           0.535933           0.253895   \n",
              "33           0.385025           0.535933           0.253995   \n",
              "34           0.384724           0.535933           0.253694   \n",
              "35           0.385025           0.535933           0.253895   \n",
              "36           0.384824           0.535933           0.253995   \n",
              "37           0.385126           0.535933           0.253995   \n",
              "38           0.384824           0.535933           0.253895   \n",
              "39           0.385126           0.535933           0.253895   \n",
              "40           0.384925           0.535933           0.254096   \n",
              "41           0.385126           0.535933           0.254096   \n",
              "42           0.384724           0.535933           0.253794   \n",
              "43           0.385126           0.535933           0.254096   \n",
              "44           0.384824           0.535933           0.253895   \n",
              "45           0.385126           0.535933           0.254096   \n",
              "46           0.384925           0.535933           0.253794   \n",
              "47           0.385025           0.535933           0.254096   \n",
              "48           0.384925           0.535933           0.254096   \n",
              "49           0.385126           0.535933           0.254096   \n",
              "50           0.384925           0.535933           0.253895   \n",
              "51           0.385126           0.535933           0.254096   \n",
              "52           0.385126           0.535933           0.253995   \n",
              "53           0.385025           0.535933           0.254096   \n",
              "54           0.385025           0.535933           0.253995   \n",
              "55           0.385126           0.535933           0.254096   \n",
              "56           0.385025           0.535933           0.254096   \n",
              "57           0.384925           0.535933           0.254297   \n",
              "58           0.385025           0.535933           0.253995   \n",
              "59           0.384925           0.535933           0.254196   \n",
              "\n",
              "    split4_test_score  split5_test_score  split6_test_score  \\\n",
              "0            0.195799           0.370892           0.300935   \n",
              "1            0.195698           0.370791           0.300734   \n",
              "2            0.196100           0.370791           0.301035   \n",
              "3            0.196402           0.370992           0.300834   \n",
              "4            0.195899           0.370691           0.300935   \n",
              "5            0.196301           0.370892           0.300834   \n",
              "6            0.196100           0.370892           0.301035   \n",
              "7            0.196301           0.370892           0.300935   \n",
              "8            0.195899           0.370892           0.301035   \n",
              "9            0.196301           0.370892           0.300834   \n",
              "10           0.196301           0.370992           0.301035   \n",
              "11           0.196402           0.370892           0.300935   \n",
              "12           0.196301           0.370992           0.301136   \n",
              "13           0.196301           0.370892           0.300834   \n",
              "14           0.196402           0.370992           0.301035   \n",
              "15           0.196402           0.370892           0.300834   \n",
              "16           0.196201           0.370992           0.301136   \n",
              "17           0.196201           0.370892           0.300734   \n",
              "18           0.196502           0.370992           0.301136   \n",
              "19           0.196804           0.370892           0.300834   \n",
              "20           0.196502           0.370992           0.301236   \n",
              "21           0.196703           0.370992           0.300834   \n",
              "22           0.196904           0.370992           0.301136   \n",
              "23           0.197005           0.370892           0.300734   \n",
              "24           0.196804           0.370892           0.301136   \n",
              "25           0.196804           0.370992           0.300734   \n",
              "26           0.197206           0.370992           0.301136   \n",
              "27           0.196804           0.370892           0.300834   \n",
              "28           0.197005           0.370892           0.301136   \n",
              "29           0.196804           0.370992           0.300734   \n",
              "30           0.197105           0.370791           0.301136   \n",
              "31           0.197206           0.370892           0.300935   \n",
              "32           0.197005           0.370791           0.301035   \n",
              "33           0.196904           0.370992           0.300935   \n",
              "34           0.197306           0.370791           0.301035   \n",
              "35           0.197005           0.370992           0.301035   \n",
              "36           0.197306           0.370992           0.300834   \n",
              "37           0.197005           0.370992           0.300834   \n",
              "38           0.197608           0.370892           0.300834   \n",
              "39           0.197407           0.370892           0.300935   \n",
              "40           0.197507           0.370791           0.300734   \n",
              "41           0.197407           0.370992           0.300834   \n",
              "42           0.197708           0.370892           0.300734   \n",
              "43           0.197809           0.370892           0.301035   \n",
              "44           0.197608           0.370590           0.300633   \n",
              "45           0.197708           0.370992           0.300935   \n",
              "46           0.197909           0.370992           0.300834   \n",
              "47           0.198211           0.370892           0.301035   \n",
              "48           0.197909           0.371193           0.300935   \n",
              "49           0.198110           0.370892           0.300935   \n",
              "50           0.197909           0.370992           0.300935   \n",
              "51           0.198211           0.370992           0.301035   \n",
              "52           0.198010           0.370892           0.300935   \n",
              "53           0.198110           0.370892           0.300834   \n",
              "54           0.198010           0.370992           0.300935   \n",
              "55           0.198512           0.370992           0.300834   \n",
              "56           0.197909           0.370892           0.300834   \n",
              "57           0.198512           0.370992           0.300734   \n",
              "58           0.198110           0.370892           0.300834   \n",
              "59           0.198512           0.370892           0.300935   \n",
              "\n",
              "    split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
              "0            0.203840           0.194190           0.535833         0.332770   \n",
              "1            0.203840           0.194793           0.535833         0.332780   \n",
              "2            0.203840           0.194291           0.535833         0.332800   \n",
              "3            0.203940           0.194492           0.535833         0.332840   \n",
              "4            0.203840           0.194391           0.535833         0.332780   \n",
              "5            0.203840           0.194592           0.535833         0.332870   \n",
              "6            0.203940           0.194391           0.535833         0.332850   \n",
              "7            0.203940           0.194592           0.535833         0.332870   \n",
              "8            0.203940           0.194492           0.535833         0.332830   \n",
              "9            0.203940           0.194592           0.535833         0.332870   \n",
              "10           0.203940           0.194391           0.535833         0.332860   \n",
              "11           0.203940           0.194592           0.535833         0.332880   \n",
              "12           0.204041           0.194693           0.535833         0.332931   \n",
              "13           0.203940           0.194592           0.535833         0.332850   \n",
              "14           0.203940           0.194492           0.535833         0.332890   \n",
              "15           0.203940           0.194592           0.535833         0.332850   \n",
              "16           0.204041           0.194793           0.535833         0.332921   \n",
              "17           0.203940           0.194592           0.535833         0.332860   \n",
              "18           0.204041           0.194793           0.535833         0.332951   \n",
              "19           0.203940           0.194492           0.535833         0.332900   \n",
              "20           0.204041           0.194492           0.535833         0.332910   \n",
              "21           0.203840           0.194291           0.535833         0.332900   \n",
              "22           0.204041           0.194492           0.535833         0.332951   \n",
              "23           0.203940           0.194391           0.535833         0.332931   \n",
              "24           0.204041           0.194592           0.535833         0.332941   \n",
              "25           0.203840           0.194291           0.535833         0.332880   \n",
              "26           0.203940           0.194190           0.535833         0.332941   \n",
              "27           0.203940           0.194492           0.535833         0.332921   \n",
              "28           0.204041           0.194391           0.535833         0.332961   \n",
              "29           0.203940           0.194391           0.535833         0.332941   \n",
              "30           0.203840           0.194291           0.535833         0.332921   \n",
              "31           0.203940           0.194492           0.535833         0.333011   \n",
              "32           0.203940           0.194592           0.535833         0.332961   \n",
              "33           0.203840           0.194391           0.535833         0.332971   \n",
              "34           0.203940           0.194592           0.535833         0.332971   \n",
              "35           0.203840           0.194391           0.535833         0.332981   \n",
              "36           0.203840           0.194592           0.535833         0.333001   \n",
              "37           0.203940           0.194492           0.535833         0.333001   \n",
              "38           0.203940           0.194592           0.535833         0.333021   \n",
              "39           0.203840           0.194391           0.535833         0.333011   \n",
              "40           0.203840           0.194391           0.535833         0.332991   \n",
              "41           0.203840           0.194391           0.535833         0.333031   \n",
              "42           0.203940           0.194492           0.535833         0.332991   \n",
              "43           0.203840           0.194291           0.535833         0.333071   \n",
              "44           0.203940           0.194693           0.535833         0.332981   \n",
              "45           0.203940           0.194190           0.535833         0.333061   \n",
              "46           0.204041           0.194492           0.535833         0.333061   \n",
              "47           0.203940           0.194190           0.535833         0.333101   \n",
              "48           0.204141           0.194693           0.535833         0.333152   \n",
              "49           0.203840           0.194291           0.535833         0.333091   \n",
              "50           0.203940           0.194391           0.535833         0.333061   \n",
              "51           0.203840           0.194291           0.535833         0.333122   \n",
              "52           0.204041           0.194592           0.535833         0.333122   \n",
              "53           0.203940           0.194291           0.535833         0.333081   \n",
              "54           0.204041           0.194492           0.535833         0.333111   \n",
              "55           0.203840           0.194190           0.535833         0.333122   \n",
              "56           0.203739           0.194492           0.535833         0.333061   \n",
              "57           0.203940           0.194391           0.535833         0.333142   \n",
              "58           0.204041           0.194391           0.535833         0.333091   \n",
              "59           0.203840           0.194391           0.535833         0.333132   \n",
              "\n",
              "    std_test_score  rank_test_score  \n",
              "0         0.122028               60  \n",
              "1         0.121979               58  \n",
              "2         0.121973               57  \n",
              "3         0.121930               55  \n",
              "4         0.121998               59  \n",
              "5         0.121915               49  \n",
              "6         0.121951               54  \n",
              "7         0.121904               47  \n",
              "8         0.121969               56  \n",
              "9         0.121901               47  \n",
              "10        0.121945               50  \n",
              "11        0.121904               45  \n",
              "12        0.121874               36  \n",
              "13        0.121914               52  \n",
              "14        0.121916               44  \n",
              "15        0.121920               53  \n",
              "16        0.121891               38  \n",
              "17        0.121940               51  \n",
              "18        0.121857               31  \n",
              "19        0.121895               43  \n",
              "20        0.121891               41  \n",
              "21        0.121923               42  \n",
              "22        0.121853               31  \n",
              "23        0.121878               37  \n",
              "24        0.121850               33  \n",
              "25        0.121927               46  \n",
              "26        0.121864               33  \n",
              "27        0.121893               40  \n",
              "28        0.121848               30  \n",
              "29        0.121879               35  \n",
              "30        0.121873               39  \n",
              "31        0.121819               19  \n",
              "32        0.121807               29  \n",
              "33        0.121867               28  \n",
              "34        0.121786               27  \n",
              "35        0.121860               26  \n",
              "36        0.121793               21  \n",
              "37        0.121841               22  \n",
              "38        0.121752               18  \n",
              "39        0.121819               19  \n",
              "40        0.121788               24  \n",
              "41        0.121811               17  \n",
              "42        0.121758               23  \n",
              "43        0.121770               12  \n",
              "44        0.121737               25  \n",
              "45        0.121787               16  \n",
              "46        0.121734               13  \n",
              "47        0.121722                8  \n",
              "48        0.121684                1  \n",
              "49        0.121739               10  \n",
              "50        0.121747               13  \n",
              "51        0.121728                4  \n",
              "52        0.121701                4  \n",
              "53        0.121726               11  \n",
              "54        0.121711                7  \n",
              "55        0.121712                4  \n",
              "56        0.121747               15  \n",
              "57        0.121659                2  \n",
              "58        0.121711                9  \n",
              "59        0.121668                3  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b61c83e6-413c-4cd8-90c3-6bbb237212a7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_n_neighbors</th>\n",
              "      <th>param_p</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>split5_test_score</th>\n",
              "      <th>split6_test_score</th>\n",
              "      <th>split7_test_score</th>\n",
              "      <th>split8_test_score</th>\n",
              "      <th>split9_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.029992</td>\n",
              "      <td>0.004034</td>\n",
              "      <td>19.583461</td>\n",
              "      <td>6.283864</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 100, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.195799</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332770</td>\n",
              "      <td>0.122028</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.031939</td>\n",
              "      <td>0.000501</td>\n",
              "      <td>10.819235</td>\n",
              "      <td>3.837921</td>\n",
              "      <td>100</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 100, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253493</td>\n",
              "      <td>0.195698</td>\n",
              "      <td>0.370791</td>\n",
              "      <td>0.300734</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194793</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332780</td>\n",
              "      <td>0.121979</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.032260</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>18.446579</td>\n",
              "      <td>5.384533</td>\n",
              "      <td>101</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 101, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384724</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.196100</td>\n",
              "      <td>0.370791</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194291</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332800</td>\n",
              "      <td>0.121973</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.032137</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>10.734623</td>\n",
              "      <td>3.914093</td>\n",
              "      <td>101</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 101, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384724</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253392</td>\n",
              "      <td>0.196402</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332840</td>\n",
              "      <td>0.121930</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.033434</td>\n",
              "      <td>0.002098</td>\n",
              "      <td>19.247969</td>\n",
              "      <td>5.107660</td>\n",
              "      <td>102</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 102, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384925</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253493</td>\n",
              "      <td>0.195899</td>\n",
              "      <td>0.370691</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332780</td>\n",
              "      <td>0.121998</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.032943</td>\n",
              "      <td>0.000539</td>\n",
              "      <td>10.866465</td>\n",
              "      <td>3.908196</td>\n",
              "      <td>102</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 102, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253794</td>\n",
              "      <td>0.196301</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332870</td>\n",
              "      <td>0.121915</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.034106</td>\n",
              "      <td>0.001616</td>\n",
              "      <td>19.334142</td>\n",
              "      <td>5.272047</td>\n",
              "      <td>103</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 103, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253694</td>\n",
              "      <td>0.196100</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332850</td>\n",
              "      <td>0.121951</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.033272</td>\n",
              "      <td>0.001580</td>\n",
              "      <td>10.856128</td>\n",
              "      <td>3.897087</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 103, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384724</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253694</td>\n",
              "      <td>0.196301</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332870</td>\n",
              "      <td>0.121904</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.033004</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>19.182827</td>\n",
              "      <td>5.265523</td>\n",
              "      <td>104</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 104, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.195899</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332830</td>\n",
              "      <td>0.121969</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.032478</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>10.978729</td>\n",
              "      <td>4.052047</td>\n",
              "      <td>104</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 104, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384724</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253794</td>\n",
              "      <td>0.196301</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332870</td>\n",
              "      <td>0.121901</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.031386</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>19.395233</td>\n",
              "      <td>5.136494</td>\n",
              "      <td>105</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 105, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253493</td>\n",
              "      <td>0.196301</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332860</td>\n",
              "      <td>0.121945</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.031246</td>\n",
              "      <td>0.001019</td>\n",
              "      <td>10.942338</td>\n",
              "      <td>4.011360</td>\n",
              "      <td>105</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 105, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.196402</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332880</td>\n",
              "      <td>0.121904</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.030954</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>18.985566</td>\n",
              "      <td>5.250723</td>\n",
              "      <td>106</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 106, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384724</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253794</td>\n",
              "      <td>0.196301</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.301136</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194693</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332931</td>\n",
              "      <td>0.121874</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.031249</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>10.944542</td>\n",
              "      <td>3.924248</td>\n",
              "      <td>106</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 106, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384724</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.196301</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332850</td>\n",
              "      <td>0.121914</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.032097</td>\n",
              "      <td>0.001430</td>\n",
              "      <td>19.467301</td>\n",
              "      <td>5.485797</td>\n",
              "      <td>107</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 107, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.196402</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332890</td>\n",
              "      <td>0.121916</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.031195</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>10.851434</td>\n",
              "      <td>3.907510</td>\n",
              "      <td>107</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 107, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253392</td>\n",
              "      <td>0.196402</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332850</td>\n",
              "      <td>0.121920</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.031097</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>19.234978</td>\n",
              "      <td>5.670268</td>\n",
              "      <td>108</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 108, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.196201</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.301136</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194793</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332921</td>\n",
              "      <td>0.121891</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.031038</td>\n",
              "      <td>0.000718</td>\n",
              "      <td>10.880824</td>\n",
              "      <td>3.923545</td>\n",
              "      <td>108</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 108, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.196201</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300734</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332860</td>\n",
              "      <td>0.121940</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.031265</td>\n",
              "      <td>0.000813</td>\n",
              "      <td>19.494126</td>\n",
              "      <td>5.427829</td>\n",
              "      <td>109</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 109, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.196502</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.301136</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194793</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332951</td>\n",
              "      <td>0.121857</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.031351</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>10.851609</td>\n",
              "      <td>3.910496</td>\n",
              "      <td>109</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 109, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253392</td>\n",
              "      <td>0.196804</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332900</td>\n",
              "      <td>0.121895</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.031380</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>18.690243</td>\n",
              "      <td>5.247739</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 110, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384724</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253493</td>\n",
              "      <td>0.196502</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.301236</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332910</td>\n",
              "      <td>0.121891</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.031162</td>\n",
              "      <td>0.000858</td>\n",
              "      <td>10.805196</td>\n",
              "      <td>3.923366</td>\n",
              "      <td>110</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 110, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253694</td>\n",
              "      <td>0.196703</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194291</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332900</td>\n",
              "      <td>0.121923</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.030878</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>18.862524</td>\n",
              "      <td>5.248098</td>\n",
              "      <td>111</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 111, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253493</td>\n",
              "      <td>0.196904</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.301136</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332951</td>\n",
              "      <td>0.121853</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.031128</td>\n",
              "      <td>0.000392</td>\n",
              "      <td>10.835748</td>\n",
              "      <td>3.923821</td>\n",
              "      <td>111</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 111, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.197005</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300734</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332931</td>\n",
              "      <td>0.121878</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.031103</td>\n",
              "      <td>0.000703</td>\n",
              "      <td>19.340723</td>\n",
              "      <td>5.417679</td>\n",
              "      <td>112</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 112, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253493</td>\n",
              "      <td>0.196804</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.301136</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332941</td>\n",
              "      <td>0.121850</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.031559</td>\n",
              "      <td>0.001397</td>\n",
              "      <td>11.179422</td>\n",
              "      <td>4.699272</td>\n",
              "      <td>112</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 112, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253493</td>\n",
              "      <td>0.196804</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300734</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194291</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332880</td>\n",
              "      <td>0.121927</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.038680</td>\n",
              "      <td>0.001158</td>\n",
              "      <td>18.740545</td>\n",
              "      <td>4.830330</td>\n",
              "      <td>113</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 113, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253493</td>\n",
              "      <td>0.197206</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.301136</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332941</td>\n",
              "      <td>0.121864</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.039666</td>\n",
              "      <td>0.004799</td>\n",
              "      <td>10.934129</td>\n",
              "      <td>3.862349</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 113, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253493</td>\n",
              "      <td>0.196804</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332921</td>\n",
              "      <td>0.121893</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.038497</td>\n",
              "      <td>0.001357</td>\n",
              "      <td>17.596461</td>\n",
              "      <td>5.229767</td>\n",
              "      <td>114</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 114, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384925</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253593</td>\n",
              "      <td>0.197005</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.301136</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332961</td>\n",
              "      <td>0.121848</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.038245</td>\n",
              "      <td>0.001348</td>\n",
              "      <td>10.928183</td>\n",
              "      <td>3.831038</td>\n",
              "      <td>114</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 114, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253895</td>\n",
              "      <td>0.196804</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300734</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332941</td>\n",
              "      <td>0.121879</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.038136</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>17.596255</td>\n",
              "      <td>5.324058</td>\n",
              "      <td>115</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 115, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384925</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253493</td>\n",
              "      <td>0.197105</td>\n",
              "      <td>0.370791</td>\n",
              "      <td>0.301136</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194291</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332921</td>\n",
              "      <td>0.121873</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.038125</td>\n",
              "      <td>0.000618</td>\n",
              "      <td>10.955339</td>\n",
              "      <td>3.974994</td>\n",
              "      <td>115</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 115, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253895</td>\n",
              "      <td>0.197206</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333011</td>\n",
              "      <td>0.121819</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.037999</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>17.600643</td>\n",
              "      <td>5.231226</td>\n",
              "      <td>116</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 116, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384724</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253895</td>\n",
              "      <td>0.197005</td>\n",
              "      <td>0.370791</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332961</td>\n",
              "      <td>0.121807</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.038171</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>10.975556</td>\n",
              "      <td>3.899864</td>\n",
              "      <td>116</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 116, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253995</td>\n",
              "      <td>0.196904</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332971</td>\n",
              "      <td>0.121867</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.038436</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>17.825837</td>\n",
              "      <td>5.520120</td>\n",
              "      <td>117</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 117, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384724</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253694</td>\n",
              "      <td>0.197306</td>\n",
              "      <td>0.370791</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332971</td>\n",
              "      <td>0.121786</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.036961</td>\n",
              "      <td>0.003540</td>\n",
              "      <td>10.934888</td>\n",
              "      <td>3.851092</td>\n",
              "      <td>117</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 117, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253895</td>\n",
              "      <td>0.197005</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332981</td>\n",
              "      <td>0.121860</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.036069</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>18.808737</td>\n",
              "      <td>5.484093</td>\n",
              "      <td>118</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 118, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253995</td>\n",
              "      <td>0.197306</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333001</td>\n",
              "      <td>0.121793</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.036368</td>\n",
              "      <td>0.001559</td>\n",
              "      <td>10.833655</td>\n",
              "      <td>3.924031</td>\n",
              "      <td>118</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 118, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253995</td>\n",
              "      <td>0.197005</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333001</td>\n",
              "      <td>0.121841</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.035087</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>18.242149</td>\n",
              "      <td>5.573804</td>\n",
              "      <td>119</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 119, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253895</td>\n",
              "      <td>0.197608</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333021</td>\n",
              "      <td>0.121752</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.036065</td>\n",
              "      <td>0.001829</td>\n",
              "      <td>10.833819</td>\n",
              "      <td>3.931543</td>\n",
              "      <td>119</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 119, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253895</td>\n",
              "      <td>0.197407</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333011</td>\n",
              "      <td>0.121819</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.036366</td>\n",
              "      <td>0.001949</td>\n",
              "      <td>18.798248</td>\n",
              "      <td>5.151254</td>\n",
              "      <td>120</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 120, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384925</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.197507</td>\n",
              "      <td>0.370791</td>\n",
              "      <td>0.300734</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332991</td>\n",
              "      <td>0.121788</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.037036</td>\n",
              "      <td>0.001841</td>\n",
              "      <td>10.929703</td>\n",
              "      <td>3.894919</td>\n",
              "      <td>120</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 120, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.197407</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333031</td>\n",
              "      <td>0.121811</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.035491</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>18.067401</td>\n",
              "      <td>5.308730</td>\n",
              "      <td>121</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 121, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384724</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253794</td>\n",
              "      <td>0.197708</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300734</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332991</td>\n",
              "      <td>0.121758</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.035679</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>10.937693</td>\n",
              "      <td>3.891505</td>\n",
              "      <td>121</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 121, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.197809</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194291</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333071</td>\n",
              "      <td>0.121770</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.036137</td>\n",
              "      <td>0.001447</td>\n",
              "      <td>18.407905</td>\n",
              "      <td>5.660284</td>\n",
              "      <td>122</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 122, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384824</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253895</td>\n",
              "      <td>0.197608</td>\n",
              "      <td>0.370590</td>\n",
              "      <td>0.300633</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194693</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.332981</td>\n",
              "      <td>0.121737</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.036364</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>11.258909</td>\n",
              "      <td>3.995546</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 122, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.197708</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333061</td>\n",
              "      <td>0.121787</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.036839</td>\n",
              "      <td>0.002120</td>\n",
              "      <td>19.423646</td>\n",
              "      <td>5.430554</td>\n",
              "      <td>123</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 123, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384925</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253794</td>\n",
              "      <td>0.197909</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333061</td>\n",
              "      <td>0.121734</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.036172</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>11.281178</td>\n",
              "      <td>3.947256</td>\n",
              "      <td>123</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 123, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.198211</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333101</td>\n",
              "      <td>0.121722</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.036589</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>19.101065</td>\n",
              "      <td>4.889240</td>\n",
              "      <td>124</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 124, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384925</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.197909</td>\n",
              "      <td>0.371193</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.204141</td>\n",
              "      <td>0.194693</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333152</td>\n",
              "      <td>0.121684</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.035816</td>\n",
              "      <td>0.001224</td>\n",
              "      <td>11.201558</td>\n",
              "      <td>4.046468</td>\n",
              "      <td>124</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 124, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.198110</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194291</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333091</td>\n",
              "      <td>0.121739</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.035678</td>\n",
              "      <td>0.001777</td>\n",
              "      <td>18.911710</td>\n",
              "      <td>5.272432</td>\n",
              "      <td>125</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 125, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384925</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253895</td>\n",
              "      <td>0.197909</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333061</td>\n",
              "      <td>0.121747</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>0.036127</td>\n",
              "      <td>0.004063</td>\n",
              "      <td>12.011537</td>\n",
              "      <td>3.858700</td>\n",
              "      <td>125</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 125, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.198211</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.301035</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194291</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333122</td>\n",
              "      <td>0.121728</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>0.035328</td>\n",
              "      <td>0.000816</td>\n",
              "      <td>18.503945</td>\n",
              "      <td>5.186942</td>\n",
              "      <td>126</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 126, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253995</td>\n",
              "      <td>0.198010</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194592</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333122</td>\n",
              "      <td>0.121701</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>0.035407</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>11.519618</td>\n",
              "      <td>3.565437</td>\n",
              "      <td>126</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 126, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.198110</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194291</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333081</td>\n",
              "      <td>0.121726</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>0.038202</td>\n",
              "      <td>0.001662</td>\n",
              "      <td>18.486252</td>\n",
              "      <td>5.260859</td>\n",
              "      <td>127</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 127, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253995</td>\n",
              "      <td>0.198010</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333111</td>\n",
              "      <td>0.121711</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>0.038634</td>\n",
              "      <td>0.000793</td>\n",
              "      <td>11.176421</td>\n",
              "      <td>3.851463</td>\n",
              "      <td>127</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 127, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385126</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.198512</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194190</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333122</td>\n",
              "      <td>0.121712</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0.037639</td>\n",
              "      <td>0.000680</td>\n",
              "      <td>17.799359</td>\n",
              "      <td>5.427855</td>\n",
              "      <td>128</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 128, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254096</td>\n",
              "      <td>0.197909</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.203739</td>\n",
              "      <td>0.194492</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333061</td>\n",
              "      <td>0.121747</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>0.038900</td>\n",
              "      <td>0.001378</td>\n",
              "      <td>11.270238</td>\n",
              "      <td>4.138703</td>\n",
              "      <td>128</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 128, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384925</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254297</td>\n",
              "      <td>0.198512</td>\n",
              "      <td>0.370992</td>\n",
              "      <td>0.300734</td>\n",
              "      <td>0.203940</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333142</td>\n",
              "      <td>0.121659</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>0.038648</td>\n",
              "      <td>0.000555</td>\n",
              "      <td>17.931905</td>\n",
              "      <td>5.292418</td>\n",
              "      <td>129</td>\n",
              "      <td>1</td>\n",
              "      <td>{'n_neighbors': 129, 'p': 1}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.385025</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.253995</td>\n",
              "      <td>0.198110</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300834</td>\n",
              "      <td>0.204041</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333091</td>\n",
              "      <td>0.121711</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>0.039802</td>\n",
              "      <td>0.004395</td>\n",
              "      <td>11.271419</td>\n",
              "      <td>3.989344</td>\n",
              "      <td>129</td>\n",
              "      <td>2</td>\n",
              "      <td>{'n_neighbors': 129, 'p': 2}</td>\n",
              "      <td>0.351859</td>\n",
              "      <td>0.384925</td>\n",
              "      <td>0.535933</td>\n",
              "      <td>0.254196</td>\n",
              "      <td>0.198512</td>\n",
              "      <td>0.370892</td>\n",
              "      <td>0.300935</td>\n",
              "      <td>0.203840</td>\n",
              "      <td>0.194391</td>\n",
              "      <td>0.535833</td>\n",
              "      <td>0.333132</td>\n",
              "      <td>0.121668</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b61c83e6-413c-4cd8-90c3-6bbb237212a7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b61c83e6-413c-4cd8-90c3-6bbb237212a7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b61c83e6-413c-4cd8-90c3-6bbb237212a7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZcNZqWB-f4Q",
        "outputId": "8b0d98e9-21ad-481e-94a6-b1d1353980ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_neighbors': 124, 'p': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found that using k = 124 and the Manhattan distance will give the best performance. The corresponding accuracy is 0.5716. Type I error rate is 0.42 and Type II error rate is 0.32."
      ],
      "metadata": {
        "id": "Eye03bTJMmuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = RobustScaler()\n",
        "Kneigh = KNeighborsClassifier(n_neighbors = 124)\n",
        "pipe = Pipeline([(\"scaler\", scaler), (\"knn\", Kneigh)])\n",
        "    \n",
        "pipe.fit(X_train, y_train)\n",
        "  \n",
        "training_score = pipe.score(X_train, y_train)\n",
        "test_score = pipe.score(X_test, y_test) \n",
        "y_pred = pipe.predict(X_test)\n",
        "expected_y  = y_test\n",
        "print(metrics.classification_report(expected_y, y_pred))\n",
        "print(metrics.confusion_matrix(expected_y, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MP8wFdc5N-I",
        "outputId": "8008d443-9b46-4905-d8ae-32345739e1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      0.88      0.71     17604\n",
            "         1.0       0.42      0.01      0.03      3635\n",
            "         2.0       0.50      0.28      0.36     11594\n",
            "\n",
            "    accuracy                           0.57     32833\n",
            "   macro avg       0.50      0.39      0.37     32833\n",
            "weighted avg       0.54      0.57      0.51     32833\n",
            "\n",
            "[[15413    12  2179]\n",
            " [ 2440    52  1143]\n",
            " [ 8235    61  3298]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix = metrics.confusion_matrix(expected_y, y_pred)\n",
        "print(\"false positive rate for 0 is \", (1 - 0.59))\n",
        "fnr = sum(confusion_matrix[0,1:])/sum(sum(confusion_matrix[:,1:]))\n",
        "print(\"false negative rate for 0 is \", fnr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGENO3QX6hzI",
        "outputId": "6a3a77c0-e9f4-48cc-c4b3-da1dd8546850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "false positive rate for 0 is  0.42000000000000004\n",
            "false negative rate for 0 is  0.32483320978502594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt2EyRBT1w1N"
      },
      "source": [
        "- Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTW1Uj2F1099",
        "outputId": "5f482a88-0b4b-4fcf-a8d6-fee180cf664c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.561203667042305\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "svm = LinearSVC(dual=False, multi_class='ovr')\n",
        "svm.fit(X_train, y_train)\n",
        "print(svm.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "CZm_VWgy3EkY",
        "outputId": "05fef613-3550-43e4-b66c-f09f21a32112"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ac8fc3d9-4665-43a1-aefd-ef1adeabf0c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_C</th>\n",
              "      <th>param_penalty</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.229533</td>\n",
              "      <td>1.330308</td>\n",
              "      <td>0.006934</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.5</td>\n",
              "      <td>l1</td>\n",
              "      <td>{'C': 0.5, 'penalty': 'l1'}</td>\n",
              "      <td>0.552842</td>\n",
              "      <td>0.558571</td>\n",
              "      <td>0.549603</td>\n",
              "      <td>0.570057</td>\n",
              "      <td>0.573676</td>\n",
              "      <td>0.56095</td>\n",
              "      <td>0.009434</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.446257</td>\n",
              "      <td>0.059177</td>\n",
              "      <td>0.008800</td>\n",
              "      <td>0.001165</td>\n",
              "      <td>0.5</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'C': 0.5, 'penalty': 'l2'}</td>\n",
              "      <td>0.552842</td>\n",
              "      <td>0.558621</td>\n",
              "      <td>0.549603</td>\n",
              "      <td>0.569907</td>\n",
              "      <td>0.573827</td>\n",
              "      <td>0.56096</td>\n",
              "      <td>0.009444</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.208818</td>\n",
              "      <td>1.280957</td>\n",
              "      <td>0.008536</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>1</td>\n",
              "      <td>l1</td>\n",
              "      <td>{'C': 1, 'penalty': 'l1'}</td>\n",
              "      <td>0.552842</td>\n",
              "      <td>0.558621</td>\n",
              "      <td>0.549603</td>\n",
              "      <td>0.570057</td>\n",
              "      <td>0.573676</td>\n",
              "      <td>0.56096</td>\n",
              "      <td>0.009432</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.448568</td>\n",
              "      <td>0.156619</td>\n",
              "      <td>0.006926</td>\n",
              "      <td>0.000370</td>\n",
              "      <td>1</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'C': 1, 'penalty': 'l2'}</td>\n",
              "      <td>0.552842</td>\n",
              "      <td>0.558621</td>\n",
              "      <td>0.549653</td>\n",
              "      <td>0.570057</td>\n",
              "      <td>0.573827</td>\n",
              "      <td>0.56100</td>\n",
              "      <td>0.009461</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.279406</td>\n",
              "      <td>0.100009</td>\n",
              "      <td>0.007108</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>10</td>\n",
              "      <td>l1</td>\n",
              "      <td>{'C': 10, 'penalty': 'l1'}</td>\n",
              "      <td>0.552842</td>\n",
              "      <td>0.558621</td>\n",
              "      <td>0.549603</td>\n",
              "      <td>0.570057</td>\n",
              "      <td>0.573726</td>\n",
              "      <td>0.56097</td>\n",
              "      <td>0.009446</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.357787</td>\n",
              "      <td>0.097534</td>\n",
              "      <td>0.006725</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>10</td>\n",
              "      <td>l2</td>\n",
              "      <td>{'C': 10, 'penalty': 'l2'}</td>\n",
              "      <td>0.552842</td>\n",
              "      <td>0.558571</td>\n",
              "      <td>0.549703</td>\n",
              "      <td>0.569756</td>\n",
              "      <td>0.573827</td>\n",
              "      <td>0.56094</td>\n",
              "      <td>0.009394</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac8fc3d9-4665-43a1-aefd-ef1adeabf0c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac8fc3d9-4665-43a1-aefd-ef1adeabf0c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac8fc3d9-4665-43a1-aefd-ef1adeabf0c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
              "0       7.229533      1.330308         0.006934        0.000454     0.5   \n",
              "1       1.446257      0.059177         0.008800        0.001165     0.5   \n",
              "2       3.208818      1.280957         0.008536        0.002532       1   \n",
              "3       1.448568      0.156619         0.006926        0.000370       1   \n",
              "4       1.279406      0.100009         0.007108        0.000812      10   \n",
              "5       1.357787      0.097534         0.006725        0.000167      10   \n",
              "\n",
              "  param_penalty                       params  split0_test_score  \\\n",
              "0            l1  {'C': 0.5, 'penalty': 'l1'}           0.552842   \n",
              "1            l2  {'C': 0.5, 'penalty': 'l2'}           0.552842   \n",
              "2            l1    {'C': 1, 'penalty': 'l1'}           0.552842   \n",
              "3            l2    {'C': 1, 'penalty': 'l2'}           0.552842   \n",
              "4            l1   {'C': 10, 'penalty': 'l1'}           0.552842   \n",
              "5            l2   {'C': 10, 'penalty': 'l2'}           0.552842   \n",
              "\n",
              "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
              "0           0.558571           0.549603           0.570057           0.573676   \n",
              "1           0.558621           0.549603           0.569907           0.573827   \n",
              "2           0.558621           0.549603           0.570057           0.573676   \n",
              "3           0.558621           0.549653           0.570057           0.573827   \n",
              "4           0.558621           0.549603           0.570057           0.573726   \n",
              "5           0.558571           0.549703           0.569756           0.573827   \n",
              "\n",
              "   mean_test_score  std_test_score  rank_test_score  \n",
              "0          0.56095        0.009434                5  \n",
              "1          0.56096        0.009444                3  \n",
              "2          0.56096        0.009432                3  \n",
              "3          0.56100        0.009461                1  \n",
              "4          0.56097        0.009446                2  \n",
              "5          0.56094        0.009394                6  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {'penalty':['l1', 'l2'], 'C':[0.5, 1, 10]}\n",
        "grid_search = GridSearchCV(svm, parameters, cv=5)\n",
        "grid_search.fit(X, y)\n",
        "results = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy of the linear model is around 0.56, which is not a big improvement from random guess.  Since the results of linear regression and linear SVM are not very good, we hypothesized that the data may not be linearly separable. Therefore, we tend to explore models that can fit nonlinear decision boundaries, such as kernel SVMs and decision trees."
      ],
      "metadata": {
        "id": "LUVILB78Wzn3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDQ5Li9evMG9"
      },
      "source": [
        "- Non-linear SVMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaFvu4PBw-Ly"
      },
      "source": [
        "We used grid search with cross-validation to find the best kernel among polynomial, rbf, sigmoid. We found out rbf kernel gave the best result. Since it took several hours to run, we didn't include the cross-validation here. The accuracy for rbf kernel SVM is 0.568, type I error is 0.42, and Type II error is 0.30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fa2b43-b0fa-4b2e-b26a-9ac6221fe8be",
        "id": "IH9tMQcTQ52H"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5681174428166783\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "svm = SVC(kernel='rbf')\n",
        "svm.fit(X_train, y_train)\n",
        "print(svm.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['No', '<30', '>30']\n",
        "y_pred = svm.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c31b5d-5398-46bd-9b1d-2310f970a209",
        "id": "qRJFD3NEQ9Sf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.58      0.91      0.71     17604\n",
            "         <30       0.52      0.00      0.01      3635\n",
            "         >30       0.51      0.22      0.31     11594\n",
            "\n",
            "    accuracy                           0.57     32833\n",
            "   macro avg       0.53      0.38      0.34     32833\n",
            "weighted avg       0.55      0.57      0.49     32833\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "print(\"false positive rate for label 0 is \", (1 - 0.58))\n",
        "fnr = sum(confusion_matrix[0,1:])/sum(sum(confusion_matrix[:,1:]))\n",
        "print(\"false negative rate for label 0 is \", fnr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5678dbbd-d6c4-4538-ae5a-7f99269600ab",
        "id": "GQiXrHkARA1C"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "false positive rate for label 0 is  0.42000000000000004\n",
            "false negative rate for label 0 is  0.30234814671065396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPRBgFL3vjET"
      },
      "source": [
        "- Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the default decision tree is highly overfitted(accuracy was really low), we first used grid search to find the best depth of the tree to regularize our model. We found out the best max depth is 3 and the corresponding accuracy is 0.566, type I error is 0.42 and type II error is 0.32."
      ],
      "metadata": {
        "id": "5aBLvqqYReeD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTO-vxYCwPsI"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "parameters = {'max_depth': np.arange(2, 11)}\n",
        "grid_search = GridSearchCV(dt, parameters, cv=5)\n",
        "grid_search.fit(X, y)\n",
        "results = pd.DataFrame.from_dict(grid_search.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(results['param_max_depth'], results['mean_test_score'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "oWW9nxauXats",
        "outputId": "a9b8be74-5aa9-4546-98e5-4f927ccc490c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efd0e698c10>]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsw6GNUjvcpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1dde3fd-5757-4fc2-e9eb-9ef7904d42c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5662290987725763\n"
          ]
        }
      ],
      "source": [
        "# best decision tree\n",
        "decision_tree = DecisionTreeClassifier(max_depth=3)\n",
        "decision_tree.fit(X_train, y_train)\n",
        "print(decision_tree.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['No', '<30', '>30']\n",
        "y_pred = decision_tree.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwsrA1XdY3BN",
        "outputId": "aad09b28-7608-4bf4-ee8f-2cfd7426d840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.58      0.89      0.71     17604\n",
            "         <30       0.00      0.00      0.00      3635\n",
            "         >30       0.49      0.25      0.33     11594\n",
            "\n",
            "    accuracy                           0.57     32833\n",
            "   macro avg       0.36      0.38      0.35     32833\n",
            "weighted avg       0.48      0.57      0.49     32833\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "print(\"false positive rate for label 0 is \", (1 - 0.58))\n",
        "fnr = sum(confusion_matrix[0,1:])/sum(sum(confusion_matrix[:,1:]))\n",
        "print(\"false negative rate for label 0 is \", fnr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QAIbWudEWQb",
        "outputId": "70f7784e-afa5-42ce-ff42-a4c9574c6493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "false positive rate for label 0 is  0.42000000000000004\n",
            "false negative rate for label 0 is  0.3208053691275168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alternative models"
      ],
      "metadata": {
        "id": "QSuRFtJRdwi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we looked at our results, we looked at smaller datasets. Using the number of patients that had a value for 'A1Cresult', we ran the experiment on a smaller dataset of 17,000 observations. However, the results did not improve. Some results for the smaller data set can be found here: https://colab.research.google.com/drive/1YeUh6x6jcyHdSE1iFEdFMP_ncNnNRGw1?usp=sharing"
      ],
      "metadata": {
        "id": "fhK33fWed7Ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion"
      ],
      "metadata": {
        "id": "vy6UDpw6dqyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpreting the results"
      ],
      "metadata": {
        "id": "oa5ADi-ZglPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model that best performed was KNN with an accuracy of 0.5716 on the testing set with the lowest accuracy of 0.56 for MLP.\n",
        "We manually selected features mutiple times and ran numerous exeperiments with all 5 algorithms and accuracy did not improve. If the issue was simply that the data was not linearly separable, KNN and decision trees should have a much better accuracy. In the case of KNN we proved this by letting k vary in a range as large as (1, 5000) and accuracy did not improve in the dataset with ~98,000 observation and on the dataset with ~18,000 observations.\n",
        "Now in terms of miminizing type 1 and type 2 errors, KNN also has better result to the other models with a false positive rate of 0.41 and false negative rate of 0.32. However, for all 5 models the difference is not so significant that we can claim that a particular model is definitely better than the others. For softmax regression we have FP rate: 0.42, FN rate:0.358\n",
        "For MLP we have FP rate: 0.41, FN rate:0.358\n",
        "For SVM we have FP rate: 0.42, FN rate:0.32\n",
        "For decision trees we have FP rate: 0.41, FN rate:0.32"
      ],
      "metadata": {
        "id": "EudCTVg_fT-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Limitations"
      ],
      "metadata": {
        "id": "d9JehQK4gogk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the team member has a parent with diabetes and recognizes the importance of 'max_glu_serum', 'A1Cresult'. We believe having a lot of those values missing and not having the exact numerical value of the result are factors that limit how much we can learn from the dataset.\n",
        "This is also true about not having the weight of the patient as sudden changes in weight are signs of rapid progression of diabetes. The data is also not representative of the diabetic population. Minorities are often disproportionally more affected by diabetes, but the dataset consists or mostly caucasian individuals.\n",
        "Another limitation we faced was the wait time for the gridsearch and cross validation. Also, the way some variables were encoded made it hard to decide it the value was meaningful. For example the feature 'change_med' was not clear if the patient had changed the dosage or the medication name."
      ],
      "metadata": {
        "id": "fIvTF1TUhrWx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug2kCdPs8_zB"
      },
      "source": [
        "#### Ethics & Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTp5mLOByIn"
      },
      "source": [
        "\n",
        "#### Data Collection:\n",
        "\n",
        "####Our Concerns:\n",
        "1) Did we consider what biases could affect our data collection? Are there any biases in our datasets? \n",
        "\n",
        "2) How can we limit exposure of the subjects?\n",
        "\n",
        "#### Measures: \n",
        "\n",
        "We will be careful when generalizing our results such as, not making claims that our results can be generalized to the entire population. The dataset is disidentified in order to limit exposure of the subjects. Also all the subjects are aware of what data has been collected, for what purpose, and on agreement.  \n",
        "\n",
        "#### Models:\n",
        "\n",
        "#### Our Concerns:\n",
        "\n",
        "1) Are we sure that our models do not rely on unfair variables? Are we unknowingly or knowingly choosing discriminatory variables? Can we say with certainty that our Model fair?\n",
        "\n",
        "2) Can anyone easily explain and understand the decisions made by the model and the results? Is there any thing about the process or the results of the model that can confuse others?\n",
        "\n",
        "3) Have we clearly addressed any and all limitations or shortcoming of our models? Have we purposefully marginalized any limitations to make us look good? Can people clearly explain when to use our model and when not to?\n",
        "\n",
        "\n",
        "#### Analysis:\n",
        "\n",
        "#### Our Concerns:\n",
        "\n",
        "1) Are there any blindspots in our analysis? Any missing perespectives?\n",
        "\n",
        "2) Are the visualizations we are using and the statistics we calculated honest? Are we accurately communicating the results of our experiments?  \n",
        "\n",
        "3) Can this experiment and analysis be reproduced? If people reuse our model and datasets, will they come to same conclusion as us? \n",
        "\n",
        "#### Measures: \n",
        "\n",
        "We have taken steps to make sure that our data and models have no biases in them. Any statsitics or claims used in the analysis will be referenced for the readers to verify. In order to make sure that our experiment and analysis is reproducible we will make sure each step we took is clear and understandable. This will allow others to easily reproduce our results using our models and datasets.      \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion"
      ],
      "metadata": {
        "id": "5vCMlrRFhD5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, the accuracy results for the 5 different algorithms show that the dataset is not good as the predictions are not much better than random chance despite hyperparamter tuning and manual model selection. A large dataset doesn't guarantee sucess."
      ],
      "metadata": {
        "id": "VYqHPTrYoDjV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acm4doKbByIv"
      },
      "source": [
        "# Footnotes\n",
        "<a name=\"abc\"></a>1.[^](#abc):Mayo Foundation for Medical Education and Research. (2021, January 20). Type 2 diabetes. Mayo Clinic. Retrieved April 23, 2022, from https://www.mayoclinic.org/diseases-conditions/type-2-diabetes/symptoms-causes/syc-20351193#:~:text=Potential%20complications%20of%20diabetes%20and,damage%20(neuropathy)%20in%20limbs. <br> \n",
        "<a name=\"admonishnote\"></a>2.[^](#admonishnote): Ostling, S., Wyckoff, J., Ciarkowski, S. L., Pai, C.-W., Choe, H. M., Bahl, V., &amp; Gianchandani, R. (2017, March 22). The relationship between diabetes mellitus and 30-day readmission rates - clinical diabetes and endocrinology. BioMed Central. Retrieved April 23, 2022, from https://clindiabetesendo.biomedcentral.com/articles/10.1186/s40842-016-0040-x <br>\n",
        "<a name=\"wfds\"></a>3.[^](#wfds): Strack, B., DeShazo, J. P., Gennings, C., Olmo, J. L., Ventura, S., Cios, K. J., &amp; Clore, J. N. (2014, April 3). Impact of hba1c measurement on hospital readmission rates: Analysis of 70,000 clinical database patient records. BioMed Research International. Retrieved April 23, 2022, from https://doi.org/10.1155/2014/781670.\n",
        "\n",
        "<a name=\"ander\"></a>4.[^](#ander): Andersen, R., & Aday, L. A. (1978). Access to Medical Care in the U.S.: Realized and Potential. Medical Care, 16(7), 533–546. http://www.jstor.org/stable/3763653\n",
        "\n",
        "<a name=\"ziller\"></a>5.[^](#ziller):Ziller, E. (2014). Access to medical care in rural America. Rural public health: Best practices and preventive models, 11-28."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrnJFxCNqDpW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "FinalProject_group015-Sp22.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}